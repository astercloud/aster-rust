//! Property-based tests for Token Estimator
//!
//! These tests validate the correctness properties defined in the design document
//! using the proptest framework.
//!
//! **Feature: context-alignment, Property 1: Token Estimation Accuracy**
//! **Validates: Requirements 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7**

#[cfg(test)]
mod property_tests {
    use crate::context::token_estimator::TokenEstimator;
    use crate::context::types::{
        CHARS_PER_TOKEN_ASIAN, CHARS_PER_TOKEN_CODE, CHARS_PER_TOKEN_DEFAULT,
    };
    use crate::conversation::message::Message;
    use proptest::prelude::*;

    // ============================================================================
    // Strategies for generating test data
    // ============================================================================

    /// Strategy for generating pure English text
    fn english_text_strategy() -> impl Strategy<Value = String> {
        prop::collection::vec("[a-zA-Z ,.!?]{1,20}", 1..50).prop_map(|words| words.join(" "))
    }

    /// Strategy for generating Chinese text
    fn chinese_text_strategy() -> impl Strategy<Value = String> {
        prop::collection::vec(
            prop::sample::select(vec![
                "你", "好", "世", "界", "中", "国", "人", "民", "大", "学", "工", "作", "生", "活",
                "时", "间", "地", "方", "问", "题",
            ]),
            5..100,
        )
        .prop_map(|chars| chars.join(""))
    }

    /// Strategy for generating Japanese text (Hiragana)
    fn japanese_text_strategy() -> impl Strategy<Value = String> {
        prop::collection::vec(
            prop::sample::select(vec![
                "あ", "い", "う", "え", "お", "か", "き", "く", "け", "こ", "さ", "し", "す", "せ",
                "そ", "た", "ち", "つ", "て", "と",
            ]),
            5..100,
        )
        .prop_map(|chars| chars.join(""))
    }

    /// Strategy for generating Korean text
    fn korean_text_strategy() -> impl Strategy<Value = String> {
        prop::collection::vec(
            prop::sample::select(vec![
                "가", "나", "다", "라", "마", "바", "사", "아", "자", "차", "카", "타", "파", "하",
                "안", "녕", "하", "세", "요", "감",
            ]),
            5..100,
        )
        .prop_map(|chars| chars.join(""))
    }

    /// Strategy for generating code-like text
    fn code_text_strategy() -> impl Strategy<Value = String> {
        prop_oneof![
            // Rust-like code
            Just("fn main() {\n    let x = 5;\n    println!(\"{}\", x);\n}".to_string()),
            Just("pub struct Foo {\n    bar: i32,\n    baz: String,\n}".to_string()),
            Just("impl Foo {\n    pub fn new() -> Self {\n        Self { bar: 0, baz: String::new() }\n    }\n}".to_string()),
            // JavaScript-like code
            Just("function hello() {\n    const x = 5;\n    return x + 1;\n}".to_string()),
            Just("class Foo {\n    constructor() {\n        this.bar = 0;\n    }\n}".to_string()),
            // Python-like code
            Just("def hello():\n    x = 5\n    return x + 1".to_string()),
            Just("class Foo:\n    def __init__(self):\n        self.bar = 0".to_string()),
            // Code with markdown block
            Just("```rust\nfn main() {}\n```".to_string()),
            Just("```javascript\nconst x = 5;\n```".to_string()),
        ]
    }

    /// Strategy for generating text with special characters
    fn special_chars_text_strategy() -> impl Strategy<Value = String> {
        prop::collection::vec(
            prop::sample::select(vec![
                "@", "#", "$", "%", "^", "&", "*", "\\", "\"", "'", "`", "~", "hello", "world",
                "test", " ", "\n", "\t",
            ]),
            10..50,
        )
        .prop_map(|parts| parts.join(""))
    }

    /// Strategy for generating mixed content text
    fn mixed_text_strategy() -> impl Strategy<Value = String> {
        prop_oneof![
            english_text_strategy(),
            chinese_text_strategy(),
            japanese_text_strategy(),
            korean_text_strategy(),
            code_text_strategy(),
            special_chars_text_strategy(),
        ]
    }

    // ============================================================================
    // Property 1: Token Estimation Accuracy
    // ============================================================================

    /// **Property 1: Token Estimation Accuracy**
    ///
    /// *For any* text content, the token estimate SHALL be within a reasonable range
    /// based on content type:
    /// - Asian text: approximately length / 2 tokens
    /// - Code: approximately length / 3 tokens
    /// - English text: approximately length / 3.5 tokens
    /// - Special characters and newlines SHALL add additional weight
    ///
    /// **Validates: Requirements 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7**
    proptest! {
        #![proptest_config(ProptestConfig::with_cases(100))]

        /// Test that token estimation is non-negative and proportional to text length
        #[test]
        fn property_1_token_estimation_non_negative(
            text in mixed_text_strategy()
        ) {
            let tokens = TokenEstimator::estimate_tokens(&text);

            // Token count should always be non-negative
            prop_assert!(tokens >= 0, "Token count should be non-negative");

            // Empty text should have 0 tokens
            if text.is_empty() {
                prop_assert_eq!(tokens, 0, "Empty text should have 0 tokens");
            } else {
                // Non-empty text should have at least 1 token
                prop_assert!(tokens >= 1, "Non-empty text should have at least 1 token");
            }
        }

        /// Test that Asian text uses approximately 2 chars per token
        #[test]
        fn property_1_asian_text_ratio(
            text in chinese_text_strategy()
        ) {
            let tokens = TokenEstimator::estimate_tokens(&text);
            let char_count = text.chars().count();

            // Asian text should use ~2 chars per token
            // Allow for some variance due to special character weight
            let expected_min = (char_count as f64 / CHARS_PER_TOKEN_ASIAN * 0.5) as usize;
            let expected_max = (char_count as f64 / CHARS_PER_TOKEN_ASIAN * 2.0) as usize + 10;

            prop_assert!(
                tokens >= expected_min && tokens <= expected_max,
                "Asian text tokens {} should be in range [{}, {}] for {} chars",
                tokens, expected_min, expected_max, char_count
            );
        }

        /// Test that code text uses approximately 3 chars per token
        #[test]
        fn property_1_code_text_ratio(
            text in code_text_strategy()
        ) {
            let tokens = TokenEstimator::estimate_tokens(&text);
            let char_count = text.chars().count();

            // Code should use ~3 chars per token
            // Allow for variance due to special characters and newlines
            let expected_min = (char_count as f64 / CHARS_PER_TOKEN_CODE * 0.3) as usize;
            let expected_max = (char_count as f64 / CHARS_PER_TOKEN_CODE * 3.0) as usize + 20;

            prop_assert!(
                tokens >= expected_min && tokens <= expected_max,
                "Code text tokens {} should be in range [{}, {}] for {} chars",
                tokens, expected_min, expected_max, char_count
            );
        }

        /// Test that English text uses approximately 3.5 chars per token
        #[test]
        fn property_1_english_text_ratio(
            text in english_text_strategy()
        ) {
            let tokens = TokenEstimator::estimate_tokens(&text);
            let char_count = text.chars().count();

            // English text should use ~3.5 chars per token
            // Allow for variance
            let expected_min = (char_count as f64 / CHARS_PER_TOKEN_DEFAULT * 0.3) as usize;
            let expected_max = (char_count as f64 / CHARS_PER_TOKEN_DEFAULT * 3.0) as usize + 10;

            prop_assert!(
                tokens >= expected_min && tokens <= expected_max,
                "English text tokens {} should be in range [{}, {}] for {} chars",
                tokens, expected_min, expected_max, char_count
            );
        }

        /// Test that special characters add weight to token estimation
        /// Note: This tests that the special character weight calculation works,
        /// but the total token count may not always increase if the content type
        /// detection changes (e.g., from English to code).
        #[test]
        fn property_1_special_chars_add_weight(
            base_text in "[a-zA-Z ]{10,50}"  // Pure alphabetic text without special chars
        ) {
            let base_tokens = TokenEstimator::estimate_tokens(&base_text);

            // Add special characters that should add weight
            let text_with_specials = format!("{}\n\n\t\t@#$%", base_text);
            let tokens_with_specials = TokenEstimator::estimate_tokens(&text_with_specials);

            // The text with special characters should have at least as many tokens
            // (may be equal if the added chars are very few relative to base)
            // We're mainly testing that the function handles special chars without error
            // and that the result is reasonable
            prop_assert!(
                tokens_with_specials > 0,
                "Text with special chars should have positive token count"
            );

            // The difference should be reasonable (not wildly different)
            let diff = (tokens_with_specials as i64 - base_tokens as i64).abs();
            prop_assert!(
                diff < (base_tokens as i64 + 20),
                "Token difference {} should be reasonable for base {} and special {}",
                diff, base_tokens, tokens_with_specials
            );
        }

        /// Test that has_asian_chars correctly detects Asian content
        #[test]
        fn property_1_asian_detection_chinese(
            text in chinese_text_strategy()
        ) {
            prop_assert!(
                TokenEstimator::has_asian_chars(&text),
                "Chinese text should be detected as Asian"
            );
        }

        /// Test that has_asian_chars correctly detects Japanese content
        #[test]
        fn property_1_asian_detection_japanese(
            text in japanese_text_strategy()
        ) {
            prop_assert!(
                TokenEstimator::has_asian_chars(&text),
                "Japanese text should be detected as Asian"
            );
        }

        /// Test that has_asian_chars correctly detects Korean content
        #[test]
        fn property_1_asian_detection_korean(
            text in korean_text_strategy()
        ) {
            prop_assert!(
                TokenEstimator::has_asian_chars(&text),
                "Korean text should be detected as Asian"
            );
        }

        /// Test that has_asian_chars returns false for English text
        #[test]
        fn property_1_asian_detection_english(
            text in english_text_strategy()
        ) {
            prop_assert!(
                !TokenEstimator::has_asian_chars(&text),
                "English text should not be detected as Asian"
            );
        }

        /// Test that is_code correctly detects code content
        #[test]
        fn property_1_code_detection(
            text in code_text_strategy()
        ) {
            prop_assert!(
                TokenEstimator::is_code(&text),
                "Code text should be detected as code: {:?}", text
            );
        }

        /// Test that is_code returns false for plain English text without code indicators
        /// Note: We exclude text containing code keywords like "if ", "for ", "while ", etc.
        /// since those are legitimate code detection heuristics
        #[test]
        fn property_1_code_detection_english(
            text in "[a-zA-Z ]{10,100}"  // Pure alphabetic text without punctuation
        ) {
            // Skip texts that contain code keywords - these are expected to be detected as code
            let code_keywords = [
                "fn ", "def ", "function ", "class ", "const ", "let ", "var ",
                "import ", "pub ", "async ", "await ", "return ", "if ", "for ", "while "
            ];

            let contains_keyword = code_keywords.iter().any(|kw| text.contains(kw));

            if contains_keyword {
                // If text contains a code keyword, it's expected to be detected as code
                // This is correct behavior, so we skip this test case
                return Ok(());
            }

            // Plain alphabetic text without any code indicators should not be detected as code
            let is_code = TokenEstimator::is_code(&text);

            prop_assert!(
                !is_code,
                "Pure alphabetic text should not be detected as code: {:?}", text
            );
        }

        /// Test message token estimation includes overhead
        #[test]
        fn property_1_message_overhead(
            text in english_text_strategy()
        ) {
            let message = Message::user().with_text(&text);
            let message_tokens = TokenEstimator::estimate_message_tokens(&message);
            let text_tokens = TokenEstimator::estimate_tokens(&text);

            // Message tokens should include overhead (at least 4 tokens)
            prop_assert!(
                message_tokens >= text_tokens,
                "Message tokens ({}) should be >= text tokens ({})",
                message_tokens, text_tokens
            );
        }

        /// Test total tokens for message array
        #[test]
        fn property_1_total_tokens_additive(
            text1 in english_text_strategy(),
            text2 in english_text_strategy()
        ) {
            let msg1 = Message::user().with_text(&text1);
            let msg2 = Message::assistant().with_text(&text2);

            let total = TokenEstimator::estimate_total_tokens(&[msg1.clone(), msg2.clone()]);
            let individual_sum = TokenEstimator::estimate_message_tokens(&msg1)
                + TokenEstimator::estimate_message_tokens(&msg2);

            prop_assert_eq!(
                total, individual_sum,
                "Total tokens should equal sum of individual message tokens"
            );
        }
    }
}
